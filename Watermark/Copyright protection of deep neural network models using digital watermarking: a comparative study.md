# Keywords DNN、Black-box、White-box、Deep learning、Copyright protection、Digital watermarking
# Abstract
```
如今，深度学习的准确性比以往任何时候都高。
这种演变使得深度学习对于关注安全的应用程序（例如自动驾驶汽车）至关重要，并帮助消费者满足他们的大部分期望。
此外，深度神经网络 (DNN) 是用于解决多个问题的强大方法。这些问题包括医疗保健、广告、营销、计算机视觉、语音处理、自然语言处理。
DNN 在这些不同领域取得了惊人的进展，但训练此类 DNN 模型需要大量时间、大量数据，并且在大多数情况下需要大量计算步骤。
销售此类预先训练的模型是一种有利可图的商业模式。
但是，未经所有者许可共享它们是一个严重的威胁。
不幸的是，一旦模型被出售，它们就可以很容易地被复制和重新分发。
本文首先回顾了数字水印技术如何真正有助于 DNN 的版权保护。然后，提出了最新技术之间的比较研究。
此外，还提出了几种优化器来提高针对微调攻击的准确性。最后，使用多个优化器在黑盒设置下进行了多次实验，并将结果与​​ SGD 优化器进行了比较。
```

# Intruduction
迁移学习或微调是一种出色的策略，它使用户能够利用预先训练的模型，以更少的重新训练时间执行其他任务。  
因此，**迁移学习或微调的想法可能会在不久的将来引起知识产权问题**。  
此外，可能会出现一些用于销售和购买预训练模型的数字平台分布。
在这种情况下，**需要保护共享预训练模型的版权**。
设计可靠的DNN身份验证过程是一项关键挑战。对于机器学习 (ML) 社区来说，这是一个相当新的领域，
**安全社区在数字水印的概念下对这个问题进行了深入研究**。
数字水印 (Dight watermark, DW) 是一种在信号（文本、图像、视频或音频）中稳健隐藏信息以验证真实性的过程。
现有的水印方法在处理 DNN 的几种情况时并不直接灵活。
**设计有效的水印来保护 DNN 的安全变得更加困难，因为它还应该保留 DNN 模型的功能。**
因此，如果由于添加水印而修改了参数，DNN应该保留因此，如果由于添加水印而修改参数，DNN应保留执行任务的能力，例如分类、回归......等。
此外，DNN 模型所有者通常更喜欢用于**保留所有权的水印算法**，而不是使用基于矩阵权重的简单哈希函数
**保护​​预训练模型的版权是本文的主要范围**
