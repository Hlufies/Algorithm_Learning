1. GPT结构讲一下，一般用来做什么任务2.为什么模型都聚焦于decoder-only而不是encoder-only或者encoder-decoder
   ```
   主要由Transformer Decoder的block堆叠而成，去除了原本的cross-attention。
   在原文中，可以用来下游加个线性层来finetune分类任务，句子关系任务，文本相似性任务，多项选择任务。
   现在一般多用于生成任务或者包装成生成任务的以上任务。
   ```
2. 为什么模型都聚焦于decoder-only而不是encoder-only或者encoder-decoder
   ```
   在自然语言处理（NLP）中，不同的模型架构，如纯解码器（decoder-only）、纯编码器（encoder-only）和编码器-解码器（encoder-decoder）架构，各有其特定的应用场景和优势。近年来，特别是在某些任务和应用中，我们确实观察到了对于纯解码器模型的偏好。这主要是基于以下几个原因：

  1. Decoder-Only Model 的特点
  生成性任务的适应性：Decoder-only 模型（如 GPT 系列）非常适合生成任务，如文本生成、故事讲述、对话系统等。这些模型被训练为基于给定的文本（前文）生成下一段文本，使其在处理这类任务时表现出色。
  
  自回归性质：Decoder-only 模型通常是自回归的，意味着它们在生成文本时，每次生成一个词元（如一个词或一个字符），然后将其作为下一个生成步骤的输入。这种逐步生成的方式非常适合语言模型。
  
  大规模预训练的优势：随着大规模语料库的可用性和计算能力的提升，decoder-only 模型通过大规模的无监督预训练，在理解和生成自然语言方面展现了惊人的能力。
   ```

   ```
    2. Encoder-Only 和 Encoder-Decoder 模型的应用场景
      Encoder-Only Model（如 BERT）：这类模型通常更适合于理解性任务（如分类、情感分析、实体识别等）。
      它们通过编码整个输入序列并提供一个全局的理解，对于需要深入分析和理解文本的任务表现更佳。
      Encoder-Decoder Model（如 Transformer 原始架构、T5、BART）：这类模型非常适合于翻译、文本摘要等任务，其中编码器用于理解输入文本，解码器用于生成输出文本。
      这种分离的架构使得模型能够有效地处理从一种形式的输入到另一种形式的输出的转换
   ```
   ```
      1. 内存更少、速度更快，便于scaling
        解码器只架构（如GPT系列）通常比编码器-解码器架构更简单。由于架构的简化，它们通常需要更少的内存和计算资源，特别是在处理长序列时。
        这种架构简化同样意味着速度上的优势，使得模型在训练和推理时更快。
        简化的架构也更容易扩展（scaling），尤其是在进行大规模训练时，如使用更大的数据集或更复杂的模型。
        2. 无信息折损
        自回归模型（如decoder-only架构中常见的）在生成新输出时，依赖于之前生成的所有输出。这意味着在生成序列的每一步中，模型都能利用到之前所有步骤的信息，从而理论上不会有信息的丢失。
        3. Attention矩阵的特性
        在解码器只架构中，使用的是自回归Attention机制，其中Attention矩阵是一个下三角矩阵。这是因为在每一步生成时，模型只能关注到当前位置之前的输出，而不能“看到”未来的信息。
        这种下三角的Attention矩阵确保了模型在每一步都仅依赖于之前的输出，避免了信息的泄露。
        关于低秩问题，在普通的Attention机制中，的确可能存在由于多个行或列相似导致的低秩问题。然而，在自回归模型中，由于下三角矩阵的结构，这种问题得到了一定程度的缓解，因为模型的关注点在每一步都是不同的。
   ```
4. 二分类任务用gpt做的话，应该怎么做?

开放问题。
5. GPT和Bert的mask有什么区别?
6. 讲一下GPT1，2，3的区别
7. GPT-2充当reward model时，是怎么得到
分数的?
8. 生成模型的生成采样算法了解吗? 说一下8.为什么对于GPT来说，in-context-learning是有效的?
9. beam search可以手写一下吗?
