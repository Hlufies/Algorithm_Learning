常见的嵌入（Embedding）模型结构可以分为几种类型，每种类型根据其应用场景和设计目标有所不同。以下是一些广泛使用的嵌入模型结构：

1. **单层嵌入模型（Single Layer Embedding Models）**:
   - 这是最基础的嵌入模型，通常只包含一个嵌入层。
   - 应用实例：Word2Vec、GloVe（用于词嵌入）。
    ```
    向量化：在单层嵌入模型中，每个唯一的输入项（如一个单词或一个商品）都被分配一个固定大小的向量。这个向量是通过学习过程得到的，能够捕捉该项的特征。
    权重矩阵：嵌入层由一个权重矩阵组成，矩阵的每一行代表一个输入项的嵌入向量。这个矩阵的大小由词汇量（或其他类型的输入项数量）和嵌入维度决定。
    映射过程：当输入一个项时（例如一个单词），模型查找该项对应的嵌入向量。这个过程可以看作是从权重矩阵中检索对应行的操作。
    ```
    ```
    1. 词嵌入（Word Embedding）：
      应用如Word2Vec、GloVe等模型，这些模型能够将单词转换为嵌入向量。
      在这些模型中，语义上相近的单词在向量空间中通常也会彼此靠近。
    2. Entity Embedding
      用于将非文本数据（如用户ID、商品ID）映射到嵌入向量。
      在推荐系统中常用，可以帮助系统理解不同用户或商品的特性。
    3. 特征嵌入（Feature Embedding）
      用于将分类特征（如性别、国家）转换为嵌入向量。
      帮助机器学习模型更好地理解和利用这些特征。
    ```
    **优点**    
    维度降低：将高维的离散数据转换为低维的连续向量，简化了数据处理流程。  
    捕捉隐含信息：能够在向量空间中捕捉输入项之间的隐含关系。  
    灵活性高：可以根据不同的应用调整嵌入维度，平衡性能和精度。  
    **缺点**  
    无上下文信息：单层嵌入模型通常不考虑输入项的上下文信息，例如在词嵌入中，相同的词在不同的上下文中有相同的表示。  
    固定长度：每个嵌入向量的长度是固定的，这可能限制了模型捕捉更细粒度信息的能力。  

2. **序列嵌入模型（Sequence Embedding Models）**:
   - 用于处理序列数据，如文本或时间序列。
   - 结合嵌入层和循环神经网络（RNN）或长短时记忆网络（LSTM）。
   - 应用实例：用于自然语言处理中的句子或段落嵌入。

3. **图嵌入模型（Graph Embedding Models）**:
   - 旨在学习图结构数据的嵌入表示。
   - 使用图神经网络（GNN）来捕捉节点之间的关系。
   - 应用实例：社交网络分析、链接预测。

4. **深度嵌入模型（Deep Embedding Models）**:
   - 结合嵌入层和多层深度学习架构。
   - 可以使用卷积神经网络（CNN）、循环神经网络（RNN）或Transformer等。
   - 应用实例：复杂的图像和文本处理任务。

5. **双塔嵌入模型（Two-Tower Embedding Models）**:
   - 由两个独立的子网络组成，每个子网络学习输入数据的嵌入表示。
   - 通常用于推荐系统，其中一个塔处理用户数据，另一个塔处理物品数据。
   - 目的是通过计算两个嵌入的相似度来实现有效的匹配。

6. **交叉嵌入模型（Cross Embedding Models）**:
   - 设计用于同时处理两种或更多类型的输入数据。
   - 结合多个嵌入层，用于学习不同类型数据间的交互。
   - 应用实例：多模态学习，例如结合文本和图像的嵌入。

每种模型结构都有其特定的优势和应用场景。选择合适的模型结构取决于具体的任务需求、数据类型以及期望达到的性能标准。
