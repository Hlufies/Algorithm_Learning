## Bert的结构是什么?一般可以做什么任务?
1. BERT的核心模型架构由Transformer的编码器 (encoder) 层堆叠而成，它采用的是全双向 (bi-directional) 结构。
2. 分类/回归任务: 利用BERT模型输出的“pooler output”，即特殊的[CLS]标记 (放置在每个输入序列前的第一个位置)的最终输出隐藏状态，再经过一个额外的全连接层和一个tanh激活函数。在下游任务的微调过程中，[CLS]标记的隐藏状态经过一个线性投影层，然后根据具体任务通过softmax (用于多类别分类)或sigmoid(用于二分类或回归任务) 层生成最终预测。
3. 序列级任务: 对于如命名实体识别 (NER)或问答 (QA) 等需要针对输入序列中每个token给出预测的任务，利用BERT输出的“sequence output”，即每层Transformer编码器对每个token生成的隐藏状态。这些隐藏状态可以直二一一续的任务特定层，如条件随机场 (CRE) 层或简单的全连接层，以产生每个token层面的预测。
