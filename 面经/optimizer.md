# 梯度下降法(Gradient Descent)  
**梯度下降法**是最基本的一类优化器，目前主要分为三种梯度下降法：标准梯度下降法(GD, Gradient Descent)，随机梯度下降法(SGD, Stochastic Gradient Descent)及批量梯度下降法(BGD, Batch Gradient Descent)
1. 标准梯度下降法主要有两个缺点:
2. 训练速度慢：每走一步都要要计算调整下一步的方向，下山的速度变慢。在应用于大型数据集中，每输入一个样本都要更新一次参数，且每次迭代都要遍历所有的样本。会使得训练过程及其缓慢，需要花费很长时间才能得到收敛解。
容易陷入局部最优解：由于是在有限视距内寻找下山的反向。当陷入平坦的洼地，会误以为到达了山地的最低点，从而不会继续往下走。所谓的局部最优解就是鞍点。落入鞍点，梯度为0，使得模型参数不在继续更新。  
![image](https://github.com/Hlufies/Algorithm_Learning/assets/130231524/ba3e88d4-1f76-4055-8412-dd49c69e549d)  

**批量梯度下降法(BGD)**  
批量梯度下降法比标准梯度下降法训练时间短，且每次下降的方向都很正确。  
![image](https://github.com/Hlufies/Algorithm_Learning/assets/130231524/4ab8fe6f-e56a-470e-9c67-e16a77337096)

**随机梯度下降法SGD**
优点：虽然SGD需要走很多步的样子，但是对梯度的要求很低（计算梯度快）。而对于引入噪声，大量的理论和实践工作证明，只要噪声不是特别大，SGD都能很好地收敛。
应用大型数据集时，训练速度很快。比如每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。相比于标准梯度下降法的遍历全部样本，每输入一个样本更新一次参数，要快得多。  

缺点：SGD在随机选择梯度的同时会引入噪声，使得权值更新的方向不一定正确。
此外，SGD也没能单独克服局部最优解的问题。

![image](https://github.com/Hlufies/Algorithm_Learning/assets/130231524/eea891eb-ea25-43c9-82e1-43a1833e0224)

