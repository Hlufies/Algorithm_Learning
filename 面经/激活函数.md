https://blog.csdn.net/GreatXiang888/article/details/99296607
1. 激活函数的用处，为什么要用激活函数
   ```
   如果没有激励函数，在这种情况下你每一层节点的输入都是上层输出的线性函数，无论你神经网络有多少层，输出都是输入的线性组合，相当于没有隐藏层，网络的学习能力有限。
  深度学习最主要的特点就是：多层，非线性。 多层为了能够学习更多的东西；没有非线性，多层和单层没什么区别，就是简单的线性组合，连异或都解决不了。

   ```
2. sigmoid函数
   ```
     它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.
     2.1容易导致梯度消失。
     如果我们初始化神经网络的权值为 [0,1]之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象。BP算法是一个迭代算法，它的基本思想为：（1）先计算每一层的状态和激活值，直到最后一层（即信号是前向传播的）；（2）计算每一层的误差，误差的计算过程是从最后一层向前推进的（这就是反向传播算法名字的由来）；（3）更新参数（目标是误差变小）。求解梯度用链导法则。迭代前面两个步骤，直到满足停止准则（比如相邻两次迭代的误差的差别很小）。
   
     问：梯度消失和梯度爆炸？改进方法。
     解决梯度爆炸：
     a.可以通过梯度截断。通过添加正则项。
     解决梯度消失：
     a.将RNN改掉，使用LSTM等自循环和门控制机制。
     b.优化激活函数，如将sigmold改为relu
     c.使用batchnorm
     d.使用残差结构
     2.2 Sigmoid 的输出不是0均值（即zero-centered）。
     这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。
     结果：那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。（我没太看明白，点击看原文这里，或者这里）
     我的理解是，像relu函数，导数为1，输入正数输出则为正数，输入负数输出则为负数，正负情况都有。而若经过sigmoid后只有正数了，如果损失函数为二次函数y = x 2 y=x^{2}y=x 2
      ,那只能从右边进行梯度下降了，左边那一块没有用上。（梯度下降是考点，引申 sgd，batch-sgd，优缺点； 其他优化器等 查看：机器学习：各种优化器Optimizer的总结与比较）
     （面试官看你说了数据的偏移，不是0均值，可能会问你，你会哪些normalization[规范化]方法？batch norm，layer norm[头条算法岗问过]会不会？
     查看：Layer-Normalization详细解析 或者 Batch-Normalization详细解析或者看这篇 BatchNormalization…等总结）
     不过这个缺点相比梯度消失来说比较小。

   ```

Relu
