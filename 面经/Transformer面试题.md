1. 如果qk用同一个权重矩阵会怎么样（退化），并行训练原理（masked）
2. flash attention，kv cache，page attention
3. torch写multi head attention
4. 多头attention用途（子空间）
