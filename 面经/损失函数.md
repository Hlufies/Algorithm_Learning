# [损失函数](https://zhuanlan.zhihu.com/p/377799012)
https://zhuanlan.zhihu.com/p/58883095
## L1 
```
也称为Mean Absolute Error，即平均绝对误差（MAE），它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。
优点： 对离群点（Outliers）或者异常值更具有鲁棒性。
缺点： 由图可知其在0点处的导数不连续，使得求解效率低下，导致收敛速度慢；而对于较小的损失值，其梯度也同其他区间损失值的梯度一样大，所以不利于网络的学习。
```
## L2
```
优点： 收敛速度快，能够对梯度给予合适的惩罚权重，而不是“一视同仁”，使梯度更新的方向可以更加精确。

缺点： 对异常值十分敏感，梯度更新的方向很容易受离群点所主导，不具备鲁棒性。

对于L1范数和L2范数，如果异常值对于实际业务非常重要，我们可以使用MSE作为我们的损失函数；
另一方面，如果异常值仅仅表示损坏的数据，那我们应该选择MAE作为损失函数。
此外，考虑到收敛速度，在大多数的卷积神经网络中（CNN）中，我们通常会选择L2损失。
但是，还存在这样一种情形，当你的业务数据中，存在95%的数据其真实值为1000，而剩下5%的数据其真实值为10时，
如果你使用MAE去训练模型，则训练出来的模型会偏向于将所有输入数据预测成1000，
因为MAE对离群点不敏感，趋向于取中值。
而采用MSE去训练模型时，训练出来的模型会偏向于将大多数的输入数据预测成10，因为它对离群点异常敏感。
因此，大多数情况这两种回归损失函数并不适用，能否有什么办法可以同时利用这两者的优点呢？
```
## Smooth L1 Loss
## Cross Entropy 
```
交叉熵。它只是平均信息长度
```
## K-L Divergence
