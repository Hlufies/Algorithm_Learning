# [损失函数](https://zhuanlan.zhihu.com/p/377799012)
https://zhuanlan.zhihu.com/p/58883095  
https://blog.csdn.net/GreatXiang888/article/details/99293507

## L1 
```
也称为Mean Absolute Error，即平均绝对误差（MAE），它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。
优点： 对离群点（Outliers）或者异常值更具有鲁棒性。
缺点： 由图可知其在0点处的导数不连续，使得求解效率低下，导致收敛速度慢；而对于较小的损失值，其梯度也同其他区间损失值的梯度一样大，所以不利于网络的学习。
```
## L2
```
优点： 收敛速度快，能够对梯度给予合适的惩罚权重，而不是“一视同仁”，使梯度更新的方向可以更加精确。

缺点： 对异常值十分敏感，梯度更新的方向很容易受离群点所主导，不具备鲁棒性。

对于L1范数和L2范数，如果异常值对于实际业务非常重要，我们可以使用MSE作为我们的损失函数；
另一方面，如果异常值仅仅表示损坏的数据，那我们应该选择MAE作为损失函数。
此外，考虑到收敛速度，在大多数的卷积神经网络中（CNN）中，我们通常会选择L2损失。
但是，还存在这样一种情形，当你的业务数据中，存在95%的数据其真实值为1000，而剩下5%的数据其真实值为10时，
如果你使用MAE去训练模型，则训练出来的模型会偏向于将所有输入数据预测成1000，
因为MAE对离群点不敏感，趋向于取中值。
而采用MSE去训练模型时，训练出来的模型会偏向于将大多数的输入数据预测成10，因为它对离群点异常敏感。
因此，大多数情况这两种回归损失函数并不适用，能否有什么办法可以同时利用这两者的优点呢？
```
## Smooth L1 Loss
## Cross Entropy 
```
交叉熵。它只是平均信息长度
```
## K-L Divergence



下面是各个损失函数的优缺点：
以下是常见的损失函数及其变种的数学公式：

1. **0-1 Loss**:
   \( L(y, \hat{y}) = \begin{cases}
   0 & \text{if } y = \hat{y} \\
   1 & \text{if } y \ne \hat{y}
   \end{cases} \)
   
2. **熵 (Entropy)**:
   \( H(p) = -\sum_{i} p_i \log p_i \)

3. **交叉熵损失 (Cross-Entropy Loss)**:
   \( L(y, \hat{y}) = -\sum_{i} y_i \log(\hat{y}_i) \)
   
4. **Softmax Loss**:
   通常是与交叉熵损失结合使用：
   \( \hat{y}_i = \frac{e^{z_i}}{\sum_{j} e^{z_j}} \)
   \( L(y, \hat{y}) = -\sum_{i} y_i \log(\hat{y}_i) \)
   
5. **KL散度 (KL Divergence)**:
   \( D_{KL}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)} \)

6. **Hinge Loss**:
   \( L(y, \hat{y}) = \max(0, 1 - y \cdot \hat{y}) \)
   其中 \( y \) 通常为 \(\{-1, 1\}\)，\(\hat{y}\) 为模型的输出。

7. **Exponential Loss**:
   \( L(y, \hat{y}) = e^{-y \cdot \hat{y}} \)

8. **Logistic Loss**:
   \( L(y, \hat{y}) = \log(1 + e^{-y \cdot \hat{y}}) \)
   
9. **Focal Loss**:
   \( L(y, \hat{y}) = -\alpha (1 - \hat{y})^\gamma y \log(\hat{y}) \)
   其中 \(\alpha\) 和 \(\gamma\) 是超参数，\(\hat{y}\) 是模型的预测概率，\(y\) 是真实标签。

这些公式是机器学习和深度学习中常用的损失函数，用于不同的任务和目标优化。


1. **0-1 Loss**:
   - **优点**: 简单直观，直接衡量预测与实际标签是否一致。
   - **缺点**: 不可微，不适合梯度下降优化，不能提供误差大小的信息。

2. **熵 (Entropy)**:
   - **优点**: 量化了系统的不确定性或混乱程度，常用于信息论。
   - **缺点**: 主要用于衡量概率分布的纯度，不能直接用于训练模型。

3. **交叉熵损失 (Cross-Entropy Loss)**:
   - **优点**: 对概率分布的微小变化敏感，适合分类问题，尤其是多分类问题。与Softmax结合使用效果很好。
   - **缺点**: 在处理类别不平衡问题时表现较差，需要特别处理。

4. **Softmax Loss**:
   - **优点**: 将模型输出转换为概率分布，与交叉熵损失结合使用效果很好，适合多分类问题。
   - **缺点**: 对异常值敏感，在类别不平衡的情况下性能不佳。

5. **KL散度 (KL Divergence)**:
   - **优点**: 衡量两个概率分布之间的差异，常用于生成模型、变分推断等。
   - **缺点**: 非对称，不是距离度量，两个分布差异较大时效果不佳。

6. **Hinge Loss**:
   - **优点**: 常用于支持向量机（SVM），鼓励较大的决策边界间隔，适合二分类问题。
   - **缺点**: 不适合多分类问题，输出需要进行裁剪，且在模型训练初期梯度变化较小。

7. **Exponential Loss**:
   - **优点**: 常用于提升算法（boosting），如AdaBoost，能显著提升弱分类器的效果。
   - **缺点**: 对噪声和异常值非常敏感，可能导致过拟合。

8. **Logistic Loss**:
   - **优点**: 常用于逻辑回归和神经网络，适合二分类问题，对数函数使得梯度下降平稳。
   - **缺点**: 在处理类别不平衡问题时表现不佳，且可能需要额外的正则化。

9. **Focal Loss**:
   - **优点**: 设计用于处理类别不平衡问题，通过调整参数 \(\alpha\) 和 \(\gamma\) 使模型更关注难分类样本。
   - **缺点**: 需要调节超参数，可能增加计算复杂度。

每种损失函数都有其特定的应用场景和适用条件，选择合适的损失函数对于模型的性能优化非常关键。
