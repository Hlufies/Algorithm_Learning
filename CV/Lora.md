# 那么什么是秩呢？  
矩阵的秩（rank）分为行秩和列秩，行秩指的是矩阵的线性无关的行的个数，列秩同理。因为一个矩阵的行秩和列秩总是相等的，因此它们统一被叫做矩阵的秩。在机器学习中，我们通常使用一个矩阵来表示一个全连接层，但是这个全连接层往往是过参数化的，这意味着我们可以通过计算这个矩阵的秩来确定哪些特征是重要和相关的。例如在主成分分析（PCA）和奇异值分解（SVD）中，我们通过一个较低维度的表示来近似表示一个高维矩阵或数据集（图1）。换句话说，我们试图找到原始特征空间（或矩阵）中少数维度的（线性）组合，能够捕捉数据集中大部分的信息。   
# 过参数化
现在深度学习的参数动不动就有几百万，LLM的参数更是数十亿起步。许多工作[2]已经表明，深度学习的矩阵往往是过参数化的（over-parametrized）。特征的内在维度（intrinsic dimension）指的是在深度学习中的真实或潜在的低维结构或信息的维度。它表示特征中存在的有效信息的维度，与特征的实际维度可能不同。事实上许多问题的内在维度比人们认为的要小的多，而对于某个数据集，内在维度在不同参数量级的模型上差距并不大。这个内在维度指的是我们解决这个问题实际上需要的参数空间的维度，我们对模型的微调通常调整的也是这些低秩的内在维度。  
 
这个结论说明了两个现象：  
1. 一旦我们找到了足够解决问题的参数空间，再增加这个参数空间的大小并不会显著提升模型的性能。
2. 一个过参数的模型的参数空间是有压缩的空间的，这也就是LoRA的提出动机。

![image](https://github.com/Hlufies/Algorithm_Learning/assets/130231524/0bd40c12-9c0d-4005-9713-f3bc7e214f51)  
![image](https://github.com/Hlufies/Algorithm_Learning/assets/130231524/0eb832dc-b09a-4051-9821-f655db3c45f8)  

![image](https://github.com/Hlufies/Algorithm_Learning/assets/130231524/4f227cfb-8b72-4571-9df1-d6fbc6432a30)

