Batch Normalization（BN）和Dropout是深度学习中常用的两种技术，它们在模型训练和推理阶段的行为有所不同：

### Batch Normalization (BN)
**训练阶段**:
- BN层会对每个小批量（batch）的数据进行归一化处理，即减去该批量的均值并除以标准差。
- 同时，BN层还会学习两个可训练参数，即缩放因子（gamma）和偏移量（beta），用于对归一化后的数据进行缩放和平移。

**推理阶段**:
- 在推理时，BN层不再使用单独小批量的统计数据，而是使用训练阶段整个数据集的均值和方差的移动平均值（running mean和running variance）。
- 缩放因子（gamma）和偏移量（beta）参数在推理时保持不变，用于对数据进行缩放和平移。

### Dropout
**训练阶段**:
- Dropout层在训练时会随机丢弃（置零）一部分神经元的输出，丢弃的概率由超参数决定。
- 这样做可以防止模型对训练数据过拟合，因为每次前向传播时网络结构都略有不同，相当于训练了多个不同的模型。

**推理阶段**:
- 在推理时，Dropout层不执行丢弃操作，即所有神经元的输出都会被保留。
- 为了保持与训练阶段相同的期望输出，推理时通常会将神经元的输出乘以（1 - dropout rate），即训练时的丢弃概率的补数。

### 总结
- BN在训练和推理时都会对数据进行缩放和平移，但使用的统计数据不同（训练时用小批量数据，推理时用整个数据集的统计数据）。
- Dropout在训练时执行随机丢弃操作以防止过拟合，而在推理时不丢弃任何神经元，但会调整输出以匹配训练时的期望。

这两种技术都有助于提高深度学习模型的性能和泛化能力，但它们在模型的不同阶段发挥着不同的作用。
