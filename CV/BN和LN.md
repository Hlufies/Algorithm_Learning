Batch Normalization（BN）和Layer Normalization（LN）都是深度学习中用于改善训练过程的技术，但它们在实现和作用上有所不同。以下是BN和LN的主要区别：

### Batch Normalization (BN)
1. **归一化范围**：BN对每个小批量（mini-batch）的数据进行归一化处理，即减去该批量的均值并除以标准差。
2. **参数学习**：BN层会学习两个可训练参数，缩放因子（gamma）和偏移量（beta），用于对归一化后的数据进行缩放和平移。
3. **依赖性**：BN的效果依赖于小批量数据的统计特性，因此它可能在小批量大小变化时表现不稳定。
4. **适用场景**：BN最初是为卷积神经网络（CNN）设计的，但也可以用于循环神经网络（RNN）和Transformer等其他类型的网络。
5. **训练与推理**：在训练时，BN使用当前小批量的统计数据；在推理时，使用训练期间整个数据集的统计数据的移动平均值。

### Layer Normalization (LN)
1. **归一化范围**：LN对网络中每一层的激活输出进行归一化，即减去该层激活的均值并除以标准差，而不是针对每个小批量。
2. **参数学习**：与BN类似，LN也学习缩放因子和偏移量，但它们是针对每一层而不是每个小批量。
3. **依赖性**：LN不依赖于小批量的大小，因此在不同的小批量大小下都能提供更一致的行为。
4. **适用场景**：LN特别适用于RNN，因为它可以减少RNN在训练过程中的不稳定性，并且有助于缓解梯度消失或梯度爆炸问题。
5. **训练与推理**：LN在训练和推理时都使用当前层的统计数据。

### 总结
- BN通过规范化每个小批量的数据来减少内部协变量偏移，而LN通过规范化每一层的激活输出来实现这一点。
- BN依赖于小批量的统计特性，而LN不依赖于小批量大小，因此在小批量训练或推理时更加稳定。
- BN通常用于CNN，而LN特别适用于RNN，但两者都可以用于不同类型的网络结构。

这两种归一化技术各有优势，选择使用哪一种取决于具体的应用场景和网络结构。在某些情况下，研究者可能会同时使用BN和LN，以结合它们的优点。


https://zhuanlan.zhihu.com/p/74516930
