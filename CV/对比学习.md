InstDisc -> Memroy bank  -> （Monmemt）动态更新  
InvaSpread + 可学习的project -> SimCLR (更多数据增强+更大Batch)  
CPC -> 生成数据扩大负样例  
CMC -> 多传感器多模态  
Swav  -> 给定同样的一张图片，如果去生成不同的视角（views），希望可以用一个视角得到的特征去预测另外一个视角的得到的特征，因为所有的这些视角的特征按道理来说都应该是非常接近的。然后SWaV将对比学习和之前的聚类的方法合在的一起，这样做也不是偶然，因为聚类也是无监督特征表示学习的方法，而且它也希望相似的物体都聚集在一个聚类中心附近，不相似的物体推到别的聚类中心。  
BYOL-> 无负样本  
![image](https://github.com/Hlufies/Algorithm_Learning/assets/130231524/aa54389f-3d72-4b6c-bc5e-d4e7ab12da5a)

SimSiam  
SimSiam（Simple Siamese）是一种无监督表示学习方法，旨在通过自监督方式学习图像特征，特别是在不需要负样本对比或记忆库的情况下。这种方法由Facebook AI Research团队提出，是对对比学习方法的进一步简化和改进。以下是对SimSiam的详细说明：

### 主要思想

SimSiam的核心思想是通过简单的Siamese网络架构来学习有意义的图像表示，而无需负样本对比。它主要依赖于正样本对比和停止梯度（stop-gradient）技巧来避免模式崩溃（即所有图像都被映射到同一个点）。

### 工作原理

SimSiam的主要流程可以分为以下几个步骤：

1. **数据增强**：
   - 对每张输入图像进行两次不同的数据增强，生成两种视图（views）。

2. **特征提取**：
   - 使用一个神经网络（通常是卷积神经网络，如ResNet）提取每个视图的特征表示。这个神经网络在SimSiam中称为编码器（Encoder）。

3. **预测头**：
   - 在编码器之后，添加一个预测头（Predictor）来生成预测向量。预测头是一个浅层的多层感知机（MLP）。

4. **停止梯度**：
   - 对于其中一个视图的表示，应用停止梯度操作，防止梯度在反向传播时通过该路径更新。

5. **相似性测量**：
   - 计算两个视图的表示之间的相似性，使用余弦相似度作为度量标准。

### 损失函数

SimSiam的损失函数是计算预测视图与停止梯度视图之间的相似性损失。具体来说，损失函数如下：

$
\mathcal{L} = -\text{cosine\_similarity}(p_1, z_2) - \text{cosine\_similarity}(p_2, z_1)
$

其中，\( p_1 \) 和 \( p_2 \) 是两个视图通过预测头后的预测向量，\( z_1 \) 和 \( z_2 \) 是两个视图通过编码器后的特征表示，并且对 \( z_2 \) 和 \( z_1 \) 应用了停止梯度操作。

### 主要优势

1. **简单高效**：
   - SimSiam避免了负样本对比和记忆库的复杂性，简化了实现，并且在较小的批量大小下也能表现良好。

2. **避免模式崩溃**：
   - 使用停止梯度技巧有效地防止了所有图像表示崩溃到同一个点的问题。

3. **高性能**：
   - 即使在没有负样本对比的情况下，SimSiam在多种自监督学习基准测试中表现出色，接近甚至超过了某些复杂的方法。

### 应用场景

SimSiam主要应用于以下领域：
- **图像分类**：通过无监督学习获得的特征可以用于提升图像分类的性能。
- **迁移学习**：预训练特征可以用于各种下游任务，如物体检测和语义分割。
- **图像检索**：学习到的图像特征可以用于相似图像检索任务。

### 总结

SimSiam是一种简单但有效的自监督学习方法，通过避免负样本对比并使用停止梯度操作，成功地学习到了高质量的图像表示。其简单的架构和高效的性能使其在自监督学习领域中具有广泛的应用潜力。



=-====================================================================================================



# First Stage
1. [InstDisc](Unsupervised Feature Learning vissa Non-Parametric Instance Discrimination)
2. [InvaSpeed](Unsupervised Embedding Learning vis Invariant and Speading Instance Feature)
3. [CPC](Representation Learning with Contrastive Predictive Coding)
4. [CMC](Contrastive Mutiview Coding)
# Second Stage
1. [Moco v1]
2. [SimCLR v1]
3. [MoCov2]
4. [SimCLRv2]
5. [SWaV]
# Third Stage
第三阶段：不用负样本
1. [BYOL]
2. [针对BYOL的博客和他们的回应]
3. SimSiam
# Forth Stage
1. [基于Transformer]
2. [MoCov3]
3. [DINO]

<img width="626" alt="image" src="https://github.com/Hlufies/Algorithm_Learning/assets/130231524/7c690742-1cf5-4058-a6d2-a9921871df69">
<img width="1066" alt="image" src="https://github.com/Hlufies/Algorithm_Learning/assets/130231524/be3e708a-0827-482e-b96d-1bcf0f030010">

在机器学习和深度学习中，特别是涉及对比学习（Contrastive Learning）的方法中，**温度（temperature）**参数通常用于控制损失函数中相似度分数的缩放。温度参数在这类任务中的作用和影响如下：

### 温度（Temperature）的作用

温度参数通常表示为 \(\tau\)（tau），用于调整相似度分数的分布，使其在计算损失函数时更为平滑或更尖锐。具体来说，它通过缩放相似度分数来控制模型对正样本和负样本的区分能力。

**公式表示：**
在对比学习中，常用的损失函数之一是对比损失（Contrastive Loss）或InfoNCE损失，其一般形式为：

\[ \mathcal{L} = -\log \frac{\exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_j) / \tau)}{\sum_{k} \exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_k) / \tau)} \]

这里：
- \(\text{sim}(\mathbf{z}_i, \mathbf{z}_j)\) 表示样本 \(\mathbf{z}_i\) 和 \(\mathbf{z}_j\) 之间的相似度（例如，点积或余弦相似度）。
- \(\tau\) 是温度参数。

**作用机制：**
1. **缩放相似度分数：** 温度参数 \(\tau\) 缩放相似度分数，使其更加尖锐或更加平滑。当 \(\tau\) 较小（接近零）时，相似度分数的差异被放大，使得模型更注重区分最相似和最不相似的样本。当 \(\tau\) 较大时，相似度分数的差异被减小，使得模型对相似度分数的变化不那么敏感。

2. **控制正负样本对比：** 温度参数调整了正样本（相似样本）和负样本（不相似样本）之间的对比强度。合适的温度参数有助于模型更有效地学习特征表示，从而更好地区分正负样本。

### 在对比学习中的作用

在对比学习中，温度参数主要通过以下方式发挥作用：

1. **稳定训练过程：** 通过调整相似度分数的分布，温度参数可以防止梯度爆炸或梯度消失问题，从而使训练过程更加稳定。

2. **提升特征表示的质量：** 适当的温度参数可以帮助模型学习到更具判别力的特征表示，使正样本的特征更加接近，负样本的特征更加分散。

3. **平衡正负样本权重：** 温度参数可以平衡正负样本在损失函数中的权重，避免模型过度关注负样本或正样本，提升模型的泛化能力。

4. **调节模型敏感性：** 通过调节温度参数，可以控制模型对相似度分数变化的敏感性，从而影响模型对输入数据细微差异的响应。

**调节温度参数：**
在实际应用中，温度参数通常通过交叉验证或实验调整来找到最佳值，以确保模型在特定任务上的最佳性能。

### 总结

温度参数在对比学习中起着至关重要的作用，通过控制相似度分数的缩放来影响模型的训练过程和特征表示质量。合适的温度参数设置能够提高模型的稳定性、判别力和泛化能力，从而提升整体性能。



温度参数（temperature parameter）在对比学习中对相似度分数的缩放有重要影响，其大小对模型的训练效果和性能有不同的影响：

### 温度参数越大时：

1. **相似度分数平滑：**
   - **缩小相似度分数差异**：相似度分数的差异被减小，导致正样本和负样本之间的差距变得不那么显著。
   - **减少梯度波动**：模型对相似度分数变化的敏感性降低，梯度变得较小，从而使训练过程更加平滑稳定。

2. **降低模型区分能力：**
   - **减弱区分正负样本的能力**：由于相似度分数被平滑处理，模型在区分正样本和负样本时可能会变得不那么有效。
   - **特征表示不够判别**：模型学习到的特征表示可能不够紧凑或不够分散，导致在实际应用中判别能力下降。

3. **影响损失函数：**
   - **减少损失值的变化幅度**：由于缩放后的相似度分数差异较小，损失函数值的变化幅度也会变小，可能导致训练收敛速度变慢。

### 温度参数越小时：

1. **相似度分数尖锐：**
   - **放大相似度分数差异**：相似度分数的差异被放大，使得正样本和负样本之间的区分更加显著。
   - **增加梯度波动**：模型对相似度分数变化的敏感性增加，梯度可能变得较大，从而导致训练过程中的波动增加。

2. **增强模型区分能力：**
   - **增强区分正负样本的能力**：由于相似度分数被放大，模型在区分正样本和负样本时更加有效。
   - **特征表示更具判别力**：模型学习到的特征表示可能更加紧凑和分散，在实际应用中表现出更强的判别能力。

3. **影响损失函数：**
   - **增加损失值的变化幅度**：由于缩放后的相似度分数差异较大，损失函数值的变化幅度也会变大，可能导致训练收敛速度加快，但也可能引入不稳定性。

### 总结：

- **温度参数较大**：使得相似度分数差异减小，训练过程较为平滑，但可能降低模型的判别能力。
- **温度参数较小**：使得相似度分数差异增大，模型在区分正负样本时更加有效，但训练过程可能变得不稳定。

在实际应用中，选择合适的温度参数需要通过实验和验证，以找到一个平衡点，使得模型既能够有效区分正负样本，又能够保持训练过程的稳定性。


温度参数越小，对比学习效果越强，即对比学习让相似样本距离就会越近，不相似样本距离越远
若想要让样本特征分布均匀，温度参数需要适中，太大和太小都不好。
当锚点与正样本的相似度最高时，温度系数越低，loss越低。
当锚点与某个负样本的相似度最高时，温度系数越低，loss越高。
由1可知，当温度系数较高时，模型的调节相对温和，不论模型是否正确预测正样本，都会调节模型
由4可知，当温度系数较低时，模型的调节比较锐利，模型偏向尽快让学会预测正样本，学会后就几乎不再调节模型。

原文链接：https://blog.csdn.net/zhaohongfei_358/article/details/129461735
