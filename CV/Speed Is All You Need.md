# 感知优化

研究人员在整个UNet架构中实现了群组归一化（Group normalization，GN）。
这种归一化技术的工作原理是将特征图（feature map）的pipeline划分为较小的组，并对每个组进行独立的归一化，使GN对批次大小的依赖性降低，更适合于各种大小的批次和各种网络结构。
稳定扩散中的文本/图像变换器有助于对条件分布P(z|τθ(y))进行建模，这对文本到图像的生成任务至关重要。
然而，自我/交叉注意力机制在处理长序列时遇到了困难，因为它们的时间和内存复杂性是平过方的。在论文中，研究人员介绍了两种可能的优化，旨在缓解这些计算瓶颈。
# Group Norm and GULE -> 专用内核：Group Norm 和 GELU
组归一化（GN）方法的工作原理是将特征图的通道（channel）划分为更小的组，并独立地对每个组进行归一化，从而使 GN 对批大小的依赖性降低，更适合各种批大小和网络架构。该研究没有按顺序执行 reshape、取均值、求方差、归一化这些操作，而是设计了一个独特的 GPU shader 形式的内核，它可以在一个 GPU 命令中执行所有这些操作，而无需任何中间张量（tensor）。
高斯误差线性单元（GELU）作为常用的模型激活函数，包含大量数值计算，例如乘法、加法和高斯误差函数。该研究用一个专用的 shader 来整合这些数值计算及其伴随的 split 和乘法操作，使它们能够在单个 AI 作画调用中执行。
## Partially Fused Softmax
Stable Diffusion 中的文本到图像 transformer 有助于对条件分布进行建模，这对于文本到图像生成任务至关重要。然而，由于内存复杂性和时间复杂度，自 / 交叉注意力机制在处理长序列时遇到了困难。基于此，该研究提出两种优化方法，以缓解计算瓶颈。  
一方面，为了避免在大矩阵上执行整个 softmax 计算，该研究使用一个 GPU shader 来减少运算操作，大大减少了中间张量的内存占用和整体延迟，具体方法如下图 2 所示。  
另一方面，该研究采用 FlashAttention [7] 这种 IO 感知的精确注意力算法，使得高带宽内存（HBM）的访问次数少于标准注意力机制，提高了整体效率。  
![image](https://github.com/Hlufies/Algorithm_Learning/assets/130231524/c41bd503-ec89-45a9-933c-be57e8bcbc37)

## FlashAttention
具体有两种加速技术：按块递增计算即平铺、并在后向传递中重新计算注意力，将所有注意力操作融合到CUDA内核中。  
相较于标准Attention，这种方法能减少HBM（高带宽内存）访问，提高整体效率。  
不过FlashAttention内核的缓存器密集度非常高（register-intensive），所以该团队是有选择性地使用这一优化方法。  
他们在注意力矩阵d=40的Adreno GPU和Apple GPU上使用FlashAttention，其他情况下使用部分融合softmax函数。  
## Winograd 卷积
Winograd 卷积将卷积运算转换为一系列矩阵乘法。这种方法可以减少许多乘法运算，提高计算效率。但是，这样一来也会增加内存消耗和数字错误，特别是在使用较大的 tile 时。

Stable Diffusion 的主干在很大程度上依赖于 3×3 卷积层，尤其是在图像解码器中，它们占了 90% 。该研究对这一现象进行了深入分析，以探索在 3 × 3 内核卷积上使用不同 tile 大小的 Winograd 的潜在好处。研究发现 4 × 4 的 tile 大小最佳，因为它在计算效率和内存利用率之间提供了最佳平衡。
