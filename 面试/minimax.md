#一面
```
自我介绍，numpy写线性回归的sgd，（前向，反向，自定义batch）
```
[numpy写线性回归的sgd，（前向，反向，自定义batch）](https://github.com/Hlufies/Algorithm_Learning/blob/main/%E9%9D%A2%E7%BB%8F/numpy%E5%86%99%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84sgd(%E5%89%8D%E5%90%91%EF%BC%8C%E5%8F%8D%E5%90%91%EF%BC%8C%E8%87%AA%E5%AE%9A%E4%B9%89batch).md)

#二面
```
1. 问项目
2. 如果同时用两个切分粒度不同的tokenizer，怎么适配，
3. 如何平衡不同tokenizer的不同粒度（加正则项，但是有点没懂），
4. 做题：回溯搜索马在4*4棋盘到四个角的步数的期望，近似算法优化（建模成图，mcts）
```
