GPT（Generative Pre-trained Transformer）是一个基于Transformer解码器的模型，它被称为"decoder-only"的原因主要与它的设计目的和结构有关：

1. **生成任务**：GPT的主要目标是生成文本，即给定一系列输入词之后，模型需要生成连贯的文本序列。这种任务更适合使用解码器，因为解码器能够处理序列生成问题。

2. **单向语言模型**：GPT作为一个单向语言模型，它只关注从左到右的信息流，即在生成下一个词时，只考虑它之前的所有词。这种单向性使得解码器成为自然的选择，因为解码器的设计就是基于掩码的自注意力机制，能够确保在生成序列的每一步中，只有之前的位置能够影响当前位置的输出。

3. **简化模型**：在Transformer的原始论文中，编码器-解码器架构被提出用于机器翻译任务，需要同时编码源语言文本和解码目标语言文本。然而，对于GPT这样的文本生成任务，只需要解码器部分来生成文本，这样可以简化模型结构，减少计算量，并提高效率。

4. **预训练与微调**：GPT在预训练阶段使用大量的文本数据进行语言模型预训练，学习语言的分布特性。在微调阶段，GPT可以针对特定任务进行调整，但由于其主要任务是文本生成，因此不需要编码器部分来处理输入序列的编码表示。

5. **Transformer的灵活性**：Transformer架构本身具有很高的灵活性，可以单独使用编码器或解码器，或者两者结合使用。GPT的设计者选择了单独使用解码器，以适应其生成任务的需求。

因此，GPT是decoder-only模型，是因为它专注于文本生成任务，并且解码器的结构非常适合这种类型的任务。这种设计使得GPT能够有效地生成连贯和自然的文本序列。
