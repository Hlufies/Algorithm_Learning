# 文本表达：解决BERT中的各向异性方法总结
```
Sentence Embeddings：即能表征句子语义的特征向量，获取这种特征向量的方法有无监督和有监督两种.
在无监督学习中，我们首先会考虑利用预训练好的大型预训练模型获取[CLS]或对句子序列纬度做MeanPooling来得到一个输入句子的特征向量。
利用这一方法获取论文中所有句子的特征向量后传入DGCNN来抽取摘要。
```
（1）这种方法被证明了有一个致命的缺点即： **Anisotropy(各向异性)**   
（2）通俗来说就是在我们熟悉的预训练模型训练过程中会导致word embeddings的各维度的特征表示不一致。  
（3）导致我们获取的句子级别的特征向量也无法进行直接比较。  
# 目前比较流行解决这一问题的方法有
1. 线性变换：bert-flow 或 bert-whitening
```
无论是在bert中增加flow层还是对得到句子向量矩阵进行白化其本质都是通过一个线性变换来缓解Anisotropy。
```
2. 对比学习
```
先对句子进行传统的文本增广，如转译、删除、插入、调换顺序等等，
再将一个句子通过两次增广得到的新句子作为正样本对，取其他句子的增广作为负样本，
进行对比学习，模型的目标也很简单，即拉近正样本对的embeddings，同时增加与负样本的距离。
```
3. SimCSE:Simple Contrastive Learning of Sentence Embeddings
```
发现利用预训练模型中自带的Dropout mask作为“增广手段”得到的Sentence Embeddings，
其质量远好于传统的增管方法，其无监督和有监督方法无监督语义上达到SOTA。
```
