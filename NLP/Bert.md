# Bert
# [Bert Code](https://github.com/Hlufies/Algorithm_Learning/tree/main/Code/Bert)
# [Bert 各向异性]()
## 演变史
### one-hot存在的问题
```
维度灾难：容易受维数灾难的困扰，每个词语的维度就是语料库字典的长度；
离散(稀疏)问题：因为 one-Hot 中，句子向量，如果词出现则为1，没出现则为0，但是由于维度远大于句子长度，所以句子中的1远小于0的个数；
维度鸿沟问题：词语的编码往往是随机的，导致不能很好地刻画词与词之间的相似性。
```
### wordvec存在的问题
```
多义词问题:因为 word2vec 为静态方式，即训练好后，每个词表达固定；
SKIP AND CBOW
```
### FastText存在的问题
```
因为 word2vec 为静态方式，即训练好后，每个词表达固定
```
### ELMO存在的问题
```
在做序列编码任务时，使用 LSTM；
ELMo 采用双向拼接的融合特征，比Bert一体化融合特征方式弱；
```
## Bert
```
BERT（Bidirectional Encoder Representations from Transformers）是一种Transformer的双向编码器，旨在通过在左右上下文中共有的条件计算来预先训练来自无标号文本的深度双向表示。
因此，经过预先训练的BERT模型只需一个额外的输出层就可以进行微调，从而为各种自然语言处理任务生成最新模型。
这个也是我们常说的 【预训练】+【微调】
```
### Bert三个关键点
```
1. 基于 transformer 结构
2. 大量语料预训练：介绍：在包含整个维基百科的无标签号文本的大语料库中（足足有25亿字！） 和图书语料库（有8亿字）中进行预训练；优点：大语料能够覆盖更多的信息
3. 双向模型:BERT是一个“深度双向”的模型。双向意味着BERT在训练阶段从所选文本的左右上下文中汲取信息
```
```
问题：
如果仅取左上下文或右上下文来预测单词“bank”的性质，那么在两个给定示例中，至少有一个会出错；
解决方法：
在做出预测之前同时考虑左上下文和右上下文
```
### Bert表征学习
#### Bert 输入输出表征长啥样

### Bert模型压缩
```
低秩因式分解
参数共享
知识蒸馏
剪枝
量化
```
