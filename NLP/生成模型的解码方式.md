## Train: Teacher-forcing model.
## Sample: Free-run model
```
对于生成模型而言，如果生成目标是得到模型认为最优（即概率最高）的文本，
则生成时需要解决的问题可以归结为：求一个单词序列，使其生成概率达到最大.
这是一个典型的搜索问题，搜索空间大小为，其中|V| 是词表大小，T 是句子的最大长度。
得到最优解的搜索方法自然是先遍历所有可能的文本，再比较文本的生成概率，从而取得概率最高的文本，这是一种穷举搜索。
但这种方法的时间复杂度、空间复杂度都非常高，因此其它一些搜索方法，如贪心搜索、集束搜索等被广泛使用。尽管这些搜索算法通常不能得到最优解，但因简单有效而被广泛使用。
除此之外，大多数生成任务要求在保证生成文本质量的基础上达到较好的多样性，因此解码时也经常采用基于随机采样的方法。
```
### 基于搜索的解码方式
1. Greedy search
  ```
  在每个时间步 t 都选取当前概率分布中概率最大的词: y^(t) = argmax P(y|Y^(<t), X)
  直到 y^(t) 为<EOS>或达到预设最大长度时停止生成。
  贪心搜索本质上是局部最优策略，但并不能保证最终结果一定是全局最优的。由于贪心搜索在解码的任意时刻只保留一条候选序列，所以在搜索效率上，贪心搜索的复杂度显著低于穷举搜索。
  ```
2. Beam search
  ```
  集束搜索（beam search）扩大了搜索范围，对贪心搜索进行了有效改进。
  虽然集束搜索的搜索范围远远不及穷举搜索，但已经覆盖了大部分概率较高的文本，因此在搜索方法中被广泛使用。
  集束搜索有一个关键的超参数“束宽”（beam size），一般用B表示。
  集束搜索的基本流程是：
  在第一个时间步，选取当前概率最大的B个词，分别当成B个候选输出序列的第一个词；
  在之后的每个时间步，将上一时刻的输出序列与词表中每个词组合后得到概率最大的B个扩增序列作为该时间步的候选输出序列。
  在t时刻，集束搜索需要考虑所有这些集束与词表上所有单词的组合
  ```
### 基于采样的解码方式
1. 随机采样
   ```
   除以最大化生成概率为解码目标外，按概率采样的解码方法也被广泛应用，即在生成时的每一步都从当前概率分布中按照概率随机采样一个词
   ```
3. 带温度的随机采样
   <img width="525" alt="image" src="https://github.com/Hlufies/Algorithm_Learning/assets/130231524/126e35b3-fb80-47ce-ba47-9cf8cc9a2a07">

   ```
   尽管随机采样在一定程度上能避免生成重复的文本，但是，由于从整个词表中采样可能会采到与上下文无关的词，
   因此，随机采样得到的文本上下文常常不连贯。
   为了使得模型尽可能避免采样到低概率的词，一个有效的办法是设置一个名为“温度”（temperature）的参数来控制概率分布的弥散程度，
   该参数用  τ 表示，τ是一个大于0的实数。
   ```
5. Top-k
   ```
   Top-k采样近来也被广泛使用。具体来说，在每个时间步，解码器首先选择概率最高的k个词作为候选词，然后根据k个词的相对概率大小从中采出一个词作为要生成的词。
   ```
6. Top-p
   ```
   尽管Top-k采样已经能够显著提高文本生成的质量，但是对于不同的模型，常数k难以进行一致的设定。
   在概率分布比较平坦的情况下，词表中有几百个词概率都相差不大，意味着此时当前词的可能选择非常多，可能存在超过k个合理的词。
   这时如果限制仅仅从Top-k个候选词中采样，可能会增加生成重复文本的风险。
   同理，如果概率分布非常集中，意味着此时可选择的词数目非常少，如可选的词汇少于k个，则从Top-k个候选词中采样可能会采到与上下文无关的词。
   ```

-----------------------------------------------------------------------------------------------------------------------------

Code

```
def batch_sample(input_ids, model, logits_warper, **model_kwargs):
    '''
    基于transformers/generation_utils.py的自回归采样
    input_ids的形状为(batch, seq_len)
    假设model和model_kwargs已经正确初始化
    '''
    while True:
        # 准备模型输入
        model_inputs = prepare_inputs_for_generation(input_ids, **model_kwargs)
        
        # 进行一次前向传播，以获取下一个token
        outputs = model.generate_step(**model_inputs)
        
        # logits - [batch, seq_len, vocab_size]
        # 代码 next_token_logits = outputs.logits[:, -1, :] 的目的是提取每个句子最后一个词对应的logits。
        next_token_logits = outputs.logits[:, -1, :] 
        
        # 处理分布
        next_token_scores = logits_warper(input_ids, next_token_scores)
        
        # 采样
        probs = torch.nn.functional.softmax(next_token_scores, dim=-1)
        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
        
        # 更新生成的ids、模型输入和下一步的长度
        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
        model_kwargs = update_model_kwargs_for_generation(
            outputs, model_kwargs
        )
        cur_len = cur_len + 1
        
        # 省略：停止标准，如EOS和最大输出长度
        if stop_criteria_met:
            break
        
    # 省略：返回最终序列

```

<img width="516" alt="image" src="https://github.com/Hlufies/Algorithm_Learning/assets/130231524/f345f938-038f-4dcb-870b-d23e63696e6b">

```
def logits_warper_topk(input_ids, scores, top_k, filter_value=-float("Inf")):
    '''
    based on transformers/generation_logits_process.py
    '''
    # Safety check
    # 确保top_k的值不会超过logits的数量。
    top_k = min(top_k, scores.size(-1))
    
    # Remove all tokens with a probability less than the last token of the top-k
    # torch.topk returns values and corresponding indices
    # We take the smallest logit in top k largest logits and calculate the mask 
    # corresponding to the positions with logits smaller than this number 
    # indices_to_remove is of shape (batch, 1, 1)
    '''
      移除概率较低的tokens:
      indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None] 这行代码执行了几个操作：
      torch.topk(scores, top_k)[0] 获取每个序列中最高的top_k logits。
      [..., -1, None] 选择每个序列中第top_k高的logit。
      < 比较原始的scores和第top_k高的logit，找出所有低于该值的logits。
      indices_to_remove 是一个布尔矩阵，表示哪些logits应该被移除。
    
      -----------------------------------------------------------------------------------------------
      example:
      scores = [[-0.2, -1.0, 2.5, 3.0, 1.0], [0.5, 0.2, -0.3, 2.0, -1.5]]
      top-k = 3
      1. 第一个样本最高的3个logits是 [3.0, 2.5, 1.0]，第二个样本是 [2.0, 0.5, 0.2]
      2. torch.topk(scores, top_k)[0][..., -1, None] 从top-k结果中获取每个样本中最大的logit。torch.topk(scores, top_k)[1]最大样本的索引
      3. 应用 [..., -1, None] 索引将从这个结果中选择每个样本的第三高的得分，并将其变为一个新维度：[[1.0], [0.2]]
      4. score < torch.topk(scores, top_k)[0][..., -1, None]
    '''
    indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]
    
    # fill in -inf in the positions we don't want to sample 
    # so that they have probability 0 after the softmax
    '''
    scores = scores.masked_fill(indices_to_remove, filter_value) 使用filter_value（负无穷大）填充那些要移除的logits，这样在后续的softmax中这些位置的概率会变为0。
    '''
    scores = scores.masked_fill(indices_to_remove, filter_value)
    return scores
    
def logits_warper(input_ids, next_token_scores):
    return logits_warper_topk(input_ids, next_token_scores, 100)

# 时间复杂度O(|V|)
```

![image](https://github.com/Hlufies/Algorithm_Learning/assets/130231524/b3ab3652-a01a-496c-8a54-52ae884d0f77)

```
# Top-P
def logits_warper_topp(input_ids, scores, top_p, filter_value=-float("Inf")):
    '''
    基于transformers/generation_logits_process.py的Top-P采样函数
    '''
    # 对scores进行降序排序。sorted_logits是排序后的logits，sorted_indices是原始scores中的索引。
    sorted_logits, sorted_indices = torch.sort(scores, descending=True)

    # 计算前缀和：cumulative_probs是形状为(batch, vocab_size)的张量。
    # 它表示每个词的累积概率。
    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)

    # 移除累积概率高于top_p阈值的tokens（概率为0的token被保留）。
    sorted_indices_to_remove = cumulative_probs > top_p

    # 将排序后的张量恢复到原始索引。
    # indices_to_remove[b][sorted_indices[b][v]] = sorted_indices_to_remove[b][v]
    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)

    # 在我们不想采样的位置填充-inf。
    # 这样在softmax后，这些位置的概率会变成0。
    scores = scores.masked_fill(indices_to_remove, filter_value)
    return scores

def logits_warper(input_ids, next_token_scores):
    return logits_warper_topp(input_ids, next_token_scores, 0.95)
```
