## Train: Teacher-forcing model.
## Sample: Free-run model
```
对于生成模型而言，如果生成目标是得到模型认为最优（即概率最高）的文本，
则生成时需要解决的问题可以归结为：求一个单词序列，使其生成概率达到最大.
这是一个典型的搜索问题，搜索空间大小为，其中|V| 是词表大小，T 是句子的最大长度。
得到最优解的搜索方法自然是先遍历所有可能的文本，再比较文本的生成概率，从而取得概率最高的文本，这是一种穷举搜索。
但这种方法的时间复杂度、空间复杂度都非常高，因此其它一些搜索方法，如贪心搜索、集束搜索等被广泛使用。尽管这些搜索算法通常不能得到最优解，但因简单有效而被广泛使用。
除此之外，大多数生成任务要求在保证生成文本质量的基础上达到较好的多样性，因此解码时也经常采用基于随机采样的方法。
```
### 基于搜索的解码方式
1. Greedy search
  ```
  在每个时间步 t 都选取当前概率分布中概率最大的词: y^(t) = argmax P(y|Y^(<t), X)
  直到 y^(t) 为<EOS>或达到预设最大长度时停止生成。
  贪心搜索本质上是局部最优策略，但并不能保证最终结果一定是全局最优的。由于贪心搜索在解码的任意时刻只保留一条候选序列，所以在搜索效率上，贪心搜索的复杂度显著低于穷举搜索。
  ```
2. Beam search
  ```
  集束搜索（beam search）扩大了搜索范围，对贪心搜索进行了有效改进。
  虽然集束搜索的搜索范围远远不及穷举搜索，但已经覆盖了大部分概率较高的文本，因此在搜索方法中被广泛使用。
  集束搜索有一个关键的超参数“束宽”（beam size），一般用B表示。
  集束搜索的基本流程是：
  在第一个时间步，选取当前概率最大的B个词，分别当成B个候选输出序列的第一个词；
  在之后的每个时间步，将上一时刻的输出序列与词表中每个词组合后得到概率最大的B个扩增序列作为该时间步的候选输出序列。
  在t时刻，集束搜索需要考虑所有这些集束与词表上所有单词的组合
  ```
### 基于采样的解码方式
1. 随机采样
   ```
   除以最大化生成概率为解码目标外，按概率采样的解码方法也被广泛应用，即在生成时的每一步都从当前概率分布中按照概率随机采样一个词
   ```
3. 带温度的随机采样
   ```
   尽管随机采样在一定程度上能避免生成重复的文本，但是，由于从整个词表中采样可能会采到与上下文无关的词，
   因此，随机采样得到的文本上下文常常不连贯。
   为了使得模型尽可能避免采样到低概率的词，一个有效的办法是设置一个名为“温度”（temperature）的参数来控制概率分布的弥散程度，
   该参数用  τ 表示，τ是一个大于0的实数。
   ```
4. Top-k
   ```
   Top-k采样近来也被广泛使用。具体来说，在每个时间步，解码器首先选择概率最高的k个词作为候选词，然后根据k个词的相对概率大小从中采出一个词作为要生成的词。
   ```
5. Top-p
   ```
   尽管Top-k采样已经能够显著提高文本生成的质量，但是对于不同的模型，常数k难以进行一致的设定。
   在概率分布比较平坦的情况下，词表中有几百个词概率都相差不大，意味着此时当前词的可能选择非常多，可能存在超过k个合理的词。
   这时如果限制仅仅从Top-k个候选词中采样，可能会增加生成重复文本的风险。
   同理，如果概率分布非常集中，意味着此时可选择的词数目非常少，如可选的词汇少于k个，则从Top-k个候选词中采样可能会采到与上下文无关的词。
   ```
